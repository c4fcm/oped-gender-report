{
 "metadata": {
  "name": "",
  "signature": "sha256:59ccac148ed8a8819e129506a6d385f7cb686cf55bb6e6eeb7fc6e3451e90d49"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Training/Testing Classifier on Op-Ed Articles\n",
      "<hr>\n",
      "<img src=\"https://s3.amazonaws.com/static.globalvoices/img/tmpl/funders-light/berkman.png\" align=\"right\">\n",
      "<br>\n",
      "author: Sands Fish\n",
      "<br>\n",
      "org:  Berkman Center for Internet & Society\n",
      "<br>\n",
      "date:   Aug. 2014"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Library, Data, Functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "from IPython.core.display import HTML\n",
      "from collections import Counter\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import json\n",
      "import dateutil.parser\n",
      "import math\n",
      "import numexpr\n",
      "import re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Read in all UK data\n",
      "guardian = pd.read_csv('/Users/sands/Data/all_articles_2/guardian_fulltext.tsv', sep='\\t')\n",
      "telegraph = pd.read_csv('/Users/sands/Data/all_articles_2/telegraph_fulltext.tsv', sep='\\t')\n",
      "dailymail = pd.read_csv('/Users/sands/Data/all_articles_2/dailymail_fulltext.tsv', sep='\\t')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Function to extract word features into format for NTLK Classifier\n",
      "def word_features(article_text):\n",
      "    words = str(article_text).split()\n",
      "    return dict((word, True) for word in words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Concatenate 3 UK data-sets and except Articles Where Text, etc. == NaN"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_uk = pd.concat([guardian, dailymail, telegraph], ignore_index=True)\n",
      "clean_uk = all_uk.dropna()\n",
      "print('Out of %d UK articles, %d are clean/non-nan' % (len(all_uk), len(clean_uk)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Out of 314772 UK articles, 262631 are clean/non-nan\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Collect all examples of op-ed articles (positive label)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "oped_sections = [clean_uk[clean_uk['section'] == 'commentisfree'], clean_uk[clean_uk['section'] == 'Comment'], clean_uk[clean_uk['section'] == 'debate']]\n",
      "oped = pd.concat(oped_sections)\n",
      "pos_examples = [(word_features(s['text']), 'oped') for i, s in oped.iterrows()]\n",
      "for o in oped_sections:\n",
      "    print('Publication Op-Ed Total: %d' % (len(o)))\n",
      "print('%d Op-Ed Articles in clean UK set' % (len(oped)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Publication Op-Ed Total: 9302\n",
        "Publication Op-Ed Total: 4320\n",
        "Publication Op-Ed Total: 1789\n",
        "15411 Op-Ed Articles in clean UK set\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Sample x random non-op-ed (negative label) articles, where x == # of positive article samples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "non_oped = clean_uk.query('section != \"commentisfree\" and section != \"Comment\" and section != \"debate\"')\n",
      "rows = np.random.choice(non_oped.index.values, len(pos_examples))\n",
      "sampled_rows = non_oped.ix[rows]\n",
      "neg_examples = [(word_features(s['text']), 'non') for i, s in sampled_rows.iterrows()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Verify Sampling Counts"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Verify that sampling from the Non-Op-Ed index resulted in the correct amount of samples\n",
      "print(len(pos_examples) == len(rows) == len(sampled_rows))\n",
      "\n",
      "# Verify the Op-Ed and Non-Op-Ed Total to the entire data-set\n",
      "print(len(non_oped) + len(oped) == len(clean_uk))\n",
      "\n",
      "# Verify that the amount of negative and positive examples are balanced\n",
      "print(len(neg_examples) == len(pos_examples))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "True\n",
        "True\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Train Classifier with set of positive and negative labels"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.classify import NaiveBayesClassifier\n",
      "train_set = pos_examples + neg_examples\n",
      "classifier = NaiveBayesClassifier.train(train_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Test Classifier Prediction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predicted_label = classifier.classify(pos_examples[5][0])\n",
      "predicted_label"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "'oped'"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Evaluate Accuracy - Positive Examples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = []\n",
      "for e in pos_examples:\n",
      "    results.append(classifier.classify(e[0]))\n",
      "Counter(results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "Counter({'oped': 15363, 'non': 48})"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Evaluate Accuracy - Negative Examples"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = []\n",
      "for e in neg_examples:\n",
      "    results.append(classifier.classify(e[0]))\n",
      "Counter(results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "Counter({'oped': 7907, 'non': 7504})"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Export Data for Use By SciKit-Learn Classifiers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pos_text_labeled = [(s['text'], 'oped') for i, s in oped.iterrows()]\n",
      "neg_text_labeled = [(s['text'], 'non-oped') for i, s in sampled_rows.iterrows()]\n",
      "all_text_and_labels = pos_text_labeled + neg_text_labeled\n",
      "\n",
      "count = 0\n",
      "with open('uk_oped_classifier_data.tsv', 'wb') as oped_out:\n",
      "    for story_text, label in all_text_and_labels:\n",
      "        try:\n",
      "            oped_out.write(label + '\\t' + story_text.replace('\\t', ' ').replace('\\n', ' ').replace('\\r', ' ') + '\\n')\n",
      "        except TypeError:\n",
      "            count = count + 1\n",
      "print('%d Errors Writing' % count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 Errors Writing\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Verify output counts "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Minor discrepancy in output counts\n",
      "open_output = pd.read_csv('uk_oped_classifier_data.tsv', sep='\\t')\n",
      "print(len(open_output))\n",
      "print(len(all_text_and_labels))\n",
      "len(open_output) == len(all_text_and_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "30746\n",
        "30822\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "False"
       ]
      }
     ],
     "prompt_number": 25
    }
   ],
   "metadata": {}
  }
 ]
}