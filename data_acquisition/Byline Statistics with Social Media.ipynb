{
 "metadata": {
  "name": "",
  "signature": "sha256:344d17e1cfd39d844498fa1b1e92f6247d7852b40fc3ff82fc636bd82d8549c0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#TODO: words written by men and women\n",
      "\n",
      "import csv\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pylab as P\n",
      "import os\n",
      "\n",
      "lines = []\n",
      "\n",
      "#CHOOSE A FOLDER FROM ABOVE\n",
      "#folder = \"./test_results/30bf9fd5bb824eb49e89c8a828276348c0b1570c/\"\n",
      "folder = \"./annual_data/\"\n",
      "\n",
      "files = [\"[2013-08-01T00:00:00Z TO 2013-09-01T00:00:00Z].csv\",\n",
      "\"[2013-09-01T00:00:00Z TO 2013-10-01T00:00:00Z].csv\",\n",
      "\"[2013-10-01T00:00:00Z TO 2013-11-01T00:00:00Z].csv\",\n",
      "\"[2013-11-01T00:00:00Z TO 2013-12-01T00:00:00Z].csv\",\n",
      "\"[2013-12-01T00:00:00Z TO 2014-01-01T00:00:00Z].csv\",\n",
      "\"[2014-01-01T00:00:00Z TO 2014-02-01T00:00:00Z].csv\",\n",
      "\"[2014-02-01T00:00:00Z TO 2014-03-01T00:00:00Z].csv\",\n",
      "\"[2014-03-01T00:00:00Z TO 2014-04-01T00:00:00Z].csv\",\n",
      "\"[2014-04-01T00:00:00Z TO 2014-05-01T00:00:00Z].csv\",\n",
      "\"[2014-05-01T00:00:00Z TO 2014-06-01T00:00:00Z].csv\",\n",
      "\"[2014-06-01T00:00:00Z TO 2014-07-01T00:00:00Z].csv\",\n",
      "\"[2014-07-01T00:00:00Z TO 2014-08-01T00:00:00Z].csv\"]\n",
      "\n",
      "top = 0\n",
      "for filename in files:\n",
      "    #with open (os.path.join(folder,\"month_06_2014.csv\")) as f:\n",
      "    with open (os.path.join(folder,files[0])) as f:\n",
      "        reader = csv.DictReader(f)\n",
      "        for i, row in enumerate(reader):\n",
      "            lines.append(row)\n",
      "    lines.pop(top)\n",
      "    top = len(lines)\n",
      "\n",
      "lines[0].keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests\n",
      "import json\n",
      "\n",
      "# reduces social media metrics to a single number\n",
      "# highly reductionist, as you might expect\n",
      "class SocialMedia:\n",
      "  def facebook(self, url):\n",
      "    #res = requests.get(\"http://graph.facebook.com/\" + url)\n",
      "    res = requests.get(\"https://graph.facebook.com/fql?q=SELECT%20like_count,%20total_count,%20share_count,%20click_count,%20comment_count%20FROM%20link_stat%20WHERE%20url%20=%20%22{0}%22\".format(url.replace(\"http://\",\"\")))\n",
      "    j = json.loads(res.text)\n",
      "    if 'data' in j.keys() and len(j['data'])>0:\n",
      "        return j['data'][0]['total_count']\n",
      "    return None\n",
      "\n",
      "  def twitter(self, url):\n",
      "    res = requests.get(\"http://urls.api.twitter.com/1/urls/count.json?url=\" + url)\n",
      "    j = json.loads(res.text)\n",
      "    if 'count' in j.keys():\n",
      "        return j['count']\n",
      "    return None\n",
      "\n",
      "  def reddit(self, url):\n",
      "    reddit_url = \"http://buttons.reddit.com/button_info.json?url={0}\".format(url)\n",
      "    res = requests.get(reddit_url)\n",
      "    #import pdb; pdb.set_trace()\n",
      "    j = json.loads(res.text)\n",
      "    if not \"data\" in j:\n",
      "      print \"REDDIT ERROR WITH {0}\".format(reddit_url)\n",
      "      return None\n",
      "      #return {\"ups\":\"0\", \"num_comments\":\"0\"}\n",
      "    else:\n",
      "      data = j['data']\n",
      "    if \"children\" in data and len(data[\"children\"]) > 0 and \"data\" in data[\"children\"][0]:\n",
      "      child = data[\"children\"][0]\n",
      "      return child['data']['ups'] + child['data']['num_comments']\n",
      "      #return {\"ups\":child[\"data\"][\"ups\"],\"num_comments\":child[\"data\"][\"num_comments\"]}\n",
      "    #return {\"ups\":\"0\", \"num_comments\":\"0\"}\n",
      "    return None\n",
      "\n",
      "sm = SocialMedia()\n",
      "print \"Facebook: {0}\".format(sm.facebook(\"http://civic.mit.edu\"))\n",
      "print \"Twitter: {0}\".format(sm.twitter(\"http://civic.mit.edu\"))\n",
      "print \"Reddit: {0}\".format(sm.reddit(\"http://civic.mit.edu\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Facebook: 268\n",
        "Twitter: 221\n",
        "Reddit: None"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#SUMMARIZE SECTION IDENTIFICATION\n",
      "MEDIA= {\n",
      "  '1': \"new york times\",\n",
      "  '2': \"washington post\",\n",
      "  '6':\"la times\",\n",
      "  '7': \"new york post\",\n",
      "  '1150': \"wall street journal\",\n",
      "  '1757': \"salon\",\n",
      "  '1707': \"daily beast\",\n",
      "  '1750': \"telegraph\",\n",
      "  '314' : \"huffington post\",\n",
      "\"27502\":\"huffington post\" #assuming these are the same for now\n",
      "}\n",
      "\n",
      "media = {}\n",
      "\n",
      "for line in lines:\n",
      "    mediakey = MEDIA[line['media_id']]\n",
      "    section = line['section']\n",
      "    if(not mediakey in media):\n",
      "        media[mediakey] = {}\n",
      "    if(not section in media[mediakey]):\n",
      "        media[mediakey][section] = 0\n",
      "    media[mediakey][section] += 1\n",
      "        \n",
      "        \n",
      "for key in media.keys():\n",
      "    articles = 0\n",
      "    for section in media[key].keys():\n",
      "        articles += media[key][section]\n",
      "    print \"{0}: {1} sections, {2} articles\".format(key,len(media[key]),articles)\n",
      "    for section in media[key].keys():\n",
      "        if(section.lower().find(\"opinion\")>=0):\n",
      "            print \"    {0}: {1}\".format(section,media[key][section])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "salon: 1 sections, 1152 articles\n",
        "huffington post: 1 sections, 149832 articles\n",
        "washington post: 12 sections, 12144 articles\n",
        "    opinion: 3516\n",
        "la times: 65 sections, 50772 articles\n",
        "    news/opinion: 1344\n",
        "    news/opinion/commentary: 720\n",
        "    OpinionLa: 1392\n",
        "new york post: 6 sections, 58500 articles\n",
        "    Opinion: 12\n",
        "new york times: 82 sections, 77100 articles\n",
        "    opinion/global: 120\n",
        "    opinion/sunday: 1056\n",
        "    Opinion: 252\n",
        "    opinion: 4596\n",
        "wall street journal: 5 sections, 51180 articles\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# GROUP BYLINES BY MEDIA ORGANISATION\n",
      "# AND SUMMARIZE\n",
      "sm = SocialMedia()\n",
      "media_people = {}\n",
      "mpop = {}\n",
      "\n",
      "from byline_gender import BylineGender\n",
      "import time\n",
      "b = BylineGender()\n",
      "        \n",
      "#for key in media.keys():\n",
      "#    articles = 0\n",
      "#    for section in media[key].keys():\n",
      "#        articles += media[key][section]\n",
      "#    print \"{0}: {1} sections, {2} articles\".format(key,len(media[key]),articles)\n",
      "#    for section in media[key].keys():\n",
      "#        if(section.lower().find(\"opinion\")>=0):\n",
      "#            print \"    {0}: {1}\".format(section,media[key][section])\n",
      "\n",
      "sections = []\n",
      "for line in lines:\n",
      "    if(line['section'].lower().find(\"opinion\")>=0):\n",
      "        section = line['section']\n",
      "        mediakey = MEDIA[line['media_id']]\n",
      "        byline_text = line['byline']    \n",
      "        \n",
      "        #just for our social media analysis:\n",
      "        if not mediakey is \"la times\":\n",
      "            continue\n",
      "       \n",
      "        social_media = [sm.facebook(line['url']),sm.twitter(line['url'])]\n",
      "        smcount = sum([y for y in social_media if not y is None])\n",
      "        sys.stdout.write(smcount+\".\")\n",
      "        time.sleep(0.25)\n",
      "       \n",
      "        if not section in sections:\n",
      "            sections.append(section)\n",
      "        for byline in b.get_full_names(byline_text):\n",
      "            if(not mediakey in media_people):\n",
      "                media_people[mediakey] = {}\n",
      "                mpop[mediakey] = {}\n",
      "            if(not byline in media_people[mediakey]):\n",
      "                media_people[mediakey][byline] = 0\n",
      "                mpop[mediakey][byline] = 0\n",
      "            media_people[mediakey][byline] += 1\n",
      "            mpop[mediakey][byline] += smcount\n",
      "            \n",
      "print \"---\"\n",
      "print sections\n",
      "for key in media_people.keys():\n",
      "    print \"{0}: {1} bylines\".format(key,len(media_people[key]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'SocialMedia' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-1-7ae5f380e6ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# GROUP BYLINES BY MEDIA ORGANISATION\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# AND SUMMARIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSocialMedia\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmedia_people\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmpop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'SocialMedia' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for key in sort(media_people.keys()):\n",
      "    print \"{0}: {1} bylines\".format(key,len(media_people[key]))\n",
      "    values = media_people[key].values()\n",
      "    plt.hist(values, max(values))\n",
      "    plt.xlabel(\"Articles Published in {0}\".format(key))\n",
      "    plt.ylabel('Number of Authors', fontsize= 20)\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from byline_gender import BylineGender\n",
      "b = BylineGender()\n",
      "b.load_name_org_online()\n",
      "unknown = []\n",
      "known = []\n",
      "\n",
      "for org in sort(media_people.keys()):\n",
      "    print \"{0}: {1} bylines\".format(org,len(media_people[org]))\n",
      "    vals = {\"female\":{},\"male\":{},\"unknown\":{}}\n",
      "    for name in media_people[org].keys():\n",
      "        #gender = b.single_name_gender(name)\n",
      "        gender = b.org_name_gender(org,name)\n",
      "        if(not gender in [\"ignore\"]):\n",
      "            vals[gender][name]=media_people[org][name]\n",
      "            if gender is \"unknown\":\n",
      "                unknown.append(name)\n",
      "            else:\n",
      "                known.append(name)    \n",
      "    m = 0\n",
      "    for v in vals.values():\n",
      "        if(len(v) > 0 and max(v)>m):\n",
      "            m = max(v)\n",
      "    \n",
      "    h = []\n",
      "    labels = []\n",
      "    for v in sort(vals.keys()):\n",
      "        labels.append(v)\n",
      "        h.append(vals[v].values())\n",
      "        if(len(h[-1]) == 0):\n",
      "            h[-1]=[0]\n",
      "    plt.figure()    \n",
      "    n,bins,patches = plt.hist(h)\n",
      "    plt.xlabel(\"Articles Published in {0}\".format(key))\n",
      "    plt.ylabel('Number of Authors', fontsize= 20)\n",
      "    legend(patches, labels)\n",
      "    plt.show()\n",
      "\n",
      "print \"UNKNOWN BYLINES: {0}\".format(len(unknown))\n",
      "print \"GUESSED BYLINES: {0}\".format(len(known))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#OUTPUT TO BYLINE FILE\n",
      "\n",
      "#f = open('org_people_upload.csv', 'w')\n",
      "#for org in sort(media_people.keys()):\n",
      "#    for name in media_people[org].keys():\n",
      "#        f.write(','.join([org.replace(\" \",\"+\"),name.replace(\" \",\"+\"),b.org_name_gender(org,name),str(media_people[org][name])])+ \"\\n\")\n",
      "#f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pct(a,b):\n",
      "    return 100*(float(a)/float(b))\n",
      "\n",
      "for org in sort(media_people.keys()):\n",
      "    print \"{0}: {1} bylines\".format(org,len(media_people[org]))\n",
      "    article_count = {\"female\":0,\"male\":0,\"unknown\":0}\n",
      "    people_count = {\"female\":0,\"male\":0,\"unknown\":0}\n",
      "    \n",
      "    total = 0\n",
      "    people_total = 0\n",
      "    for name in media_people[org].keys():\n",
      "        #gender = b.single_name_gender(name)\n",
      "        gender = b.org_name_gender(org,name)\n",
      "        if(not gender in [\"ignore\"]):\n",
      "            article_count[gender]+= media_people[org][name]\n",
      "            people_count[gender] += 1\n",
      "            people_total += 1\n",
      "            total += media_people[org][name]  \n",
      "\n",
      "    #ARTICLE COUNT CHART\n",
      "    P.figure(1, figsize=(6,6))\n",
      "    labels = 'female', 'male', 'unknown'\n",
      "    fracs = [pct(article_count['female'],total), pct(article_count['male'],total), pct(article_count['unknown'],total)]\n",
      "    explode=(0.06, 0, 0)\n",
      "    P.pie(fracs, explode=explode, labels=labels,\n",
      "                    autopct='%1.1f%%', shadow=True)\n",
      "    P.title('Author Gender per Article in {0} across {1} authors and {2} articles'.format(org,len(media_people[org]), total), bbox={'facecolor':'0.8', 'pad':5})\n",
      "    P.show()\n",
      "    \n",
      "    #PEOPLE COUNT CHART\n",
      "    P.figure(1, figsize=(6,6))\n",
      "    labels = 'female', 'male', 'unknown'\n",
      "    fracs = [pct(people_count['female'],people_total), pct(people_count['male'],people_total), pct(people_count['unknown'],people_total)]\n",
      "    explode=(0.06, 0, 0)\n",
      "    P.pie(fracs, explode=explode, labels=labels,\n",
      "                    autopct='%1.1f%%', shadow=True)\n",
      "    P.title('Unique Author Gender in {0} across {1} authors and {2} articles'.format(org,len(media_people[org]), total), bbox={'facecolor':'0.8', 'pad':5})\n",
      "    P.show()\n",
      "    \n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}